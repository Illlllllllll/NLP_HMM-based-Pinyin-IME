{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6dcb70a",
   "metadata": {},
   "source": [
    "# 03 全流程构建与解码演示\n",
    "本 Notebook 将整合此前分散的脚本，完成：\n",
    "1. 载入与聚合字典（`usrs/`：Chara.gb / TONEPY.txt / Pth.gb / HSK词性表）\n",
    "2. 生成聚合字典文件 `resources/lexicon_aggregate.json`（含 base_pinyin_to_chars）\n",
    "3. 基于 PFR 人民日报语料（示例：1998-01）统计字级 unigram / bigram + 发射计数\n",
    "4. 根据统计结果构建 HMM 参数并保存 `resources/hmm_params.json`\n",
    "5. 使用简短拼音序列做一次解码演示\n",
    "6.（可选）校验：若关键文件存在则跳过重复构建（可手动强制重建）\n",
    "\n",
    "注意：\n",
    "- 发射计数当前策略为：字出现次数分配到其所有候选 base 拼音（不是真实多音字概率，需后续改进）。\n",
    "- Pth.gb 已转为 UTF-8，本 Notebook 直接按 UTF-8 读取。\n",
    "- 若语料较大，统计可能耗时；本示例仅用 199801。\n",
    "- 可扩展新增词级 n-gram / Top-K Beam 等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13cb4bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base: E:\\zz_save\\大三上\\NLP\\Coursework 1\n",
      "Corpus exists: True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json, math, sys\n",
    "from collections import Counter\n",
    "sys.path.append('..')  # 允许导入 src.* 模块\n",
    "\n",
    "from src.preprocess.load_lexicons import load_all, tone_to_base\n",
    "from src.preprocess.build_stats import build_stats, attach_pinyin_emission\n",
    "from src.models.hmm import HMMParams\n",
    "from src.decoder.viterbi import viterbi_decode\n",
    "\n",
    "BASE_DIR = Path('..').resolve()\n",
    "USRS_DIR = BASE_DIR / 'usrs'\n",
    "RES_DIR = BASE_DIR / 'resources'\n",
    "CORPUS_FILE = USRS_DIR / 'peopledaily' / 'PeopleDaily199801.txt'\n",
    "RES_DIR.mkdir(exist_ok=True)\n",
    "print('Base:', BASE_DIR)\n",
    "print('Corpus exists:', CORPUS_FILE.exists())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215acf2e",
   "metadata": {},
   "source": [
    "## 1. 载入与聚合字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d644d560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "聚合字典已存在，已加载。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['char_to_pinyins',\n",
       " 'pinyin_tone_to_chars',\n",
       " 'base_pinyin_to_chars',\n",
       " 'word_to_pinyin_seq',\n",
       " 'hsk_word_pos']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon_path = RES_DIR / 'lexicon_aggregate.json'\n",
    "force_rebuild = False  # 如需强制重新生成设为 True\n",
    "if force_rebuild or not lexicon_path.exists():\n",
    "    lex_data = load_all(USRS_DIR)\n",
    "    lexicon_path.write_text(json.dumps(lex_data, ensure_ascii=False, indent=2), encoding='utf-8')\n",
    "    print('聚合字典已生成:', lexicon_path)\n",
    "else:\n",
    "    lex_data = json.loads(lexicon_path.read_text(encoding='utf-8'))\n",
    "    print('聚合字典已存在，已加载。')\n",
    "list(lex_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51c4f78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 候选数: 7 示例: 阿啊呵腌吖锕嗄\n",
      "ai 候选数: 23 示例: 哀挨埃唉哎捱锿呆癌皑矮蔼\n",
      "shi 候选数: 68 示例: 师诗失施尸湿狮嘘虱蓍酾鲺\n",
      "zhong 候选数: 17 示例: 中终钟忠衷锺盅忪螽舯种肿\n",
      "ren 候选数: 17 示例: 人任仁壬忍稔荏认韧刃纫饪\n"
     ]
    }
   ],
   "source": [
    "# 预览 base_pinyin_to_chars 中几个拼音的候选规模\n",
    "base_map = lex_data['base_pinyin_to_chars']\n",
    "for py in ['a','ai','shi','zhong','ren']:\n",
    "    cand = base_map.get(py, [])\n",
    "    print(py, '候选数:', len(cand), '示例:', ''.join(cand[:12]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209bf497",
   "metadata": {},
   "source": [
    "## 2. 统计 unigram / bigram 以及发射计数\n",
    "这里直接复用 `build_stats.py` 中的核心函数以避免再次写文件再读。\n",
    "（若需命令行方式，可单独运行模块：见 `resources/README.md` 示例。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f5e237d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram 字种数量: 4577 总频次: 1606385\n",
      "bigram 数量: 314074\n",
      "发射计数字数: 4557\n",
      "统计持久化完成。\n"
     ]
    }
   ],
   "source": [
    "# 统计字级频次\n",
    "unigram, bigram = build_stats(CORPUS_FILE)\n",
    "print('unigram 字种数量:', len(unigram), '总频次:', sum(unigram.values()))\n",
    "print('bigram 数量:', len(bigram))\n",
    "# 构造发射计数\n",
    "emit_counts = attach_pinyin_emission(unigram, base_map)\n",
    "print('发射计数字数:', len(emit_counts))\n",
    "# 保存到文件，保持与脚本接口一致\n",
    "def save_counter(counter, path: Path):\n",
    "    obj = {'__type__':'counter','data':{(k if not isinstance(k, tuple) else '|'.join(k)):v for k,v in counter.items()}}\n",
    "    path.write_text(json.dumps(obj, ensure_ascii=False), encoding='utf-8')\n",
    "save_counter(unigram, RES_DIR / 'freq_unigram.json')\n",
    "save_counter(bigram, RES_DIR / 'freq_bigram.json')\n",
    "(RES_DIR / 'freq_emit.json').write_text(json.dumps({ch: dict(c.items()) for ch,c in emit_counts.items()}, ensure_ascii=False), encoding='utf-8')\n",
    "print('统计持久化完成。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c564d9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 unigram:\n",
      "的 55212\n",
      "国 17901\n",
      "一 17642\n",
      "在 13701\n",
      "中 12962\n",
      "人 12586\n",
      "了 12434\n",
      "和 11945\n",
      "是 11625\n",
      "有 11193\n",
      "年 11072\n",
      "大 10966\n",
      "不 9291\n",
      "为 8886\n",
      "会 8510\n",
      "示例字: 的 发射拼音数: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('de', 55212), ('di', 55212)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 预览若干高频字\n",
    "print('Top 15 unigram:')\n",
    "for ch, cnt in unigram.most_common(15):\n",
    "    print(ch, cnt)\n",
    "# 预览某个字的发射候选（截断）\n",
    "sample_char = unigram.most_common(1)[0][0]\n",
    "emit_row = emit_counts.get(sample_char, {})\n",
    "print('示例字:', sample_char, '发射拼音数:', len(emit_row))\n",
    "list(list(emit_row.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f393ef",
   "metadata": {},
   "source": [
    "## 3. 构建 HMM 参数\n",
    "`HMMParams.from_frequency(unigram, bigram, emit)` 会：\n",
    "- init: unigram 归一化\n",
    "- trans: (a,b) / a + add-k 平滑\n",
    "- emit: (char,pinyin)/sum(char,*) + add-k 平滑\n",
    "注意：调用时顺序：unigram_path, bigram_path, emit_path（不要传错）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2691d923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM 参数已保存: E:\\zz_save\\大三上\\NLP\\Coursework 1_拼音（全拼）联想输入法\\resources\\hmm_params.json\n",
      "init 大小: 4577\n",
      "trans 起始状态数: 4577\n",
      "emit 字数: 4557\n"
     ]
    }
   ],
   "source": [
    "hmm_params_path = RES_DIR / 'hmm_params.json'\n",
    "hmm = HMMParams.from_frequency(RES_DIR / 'freq_unigram.json',\n",
    "                               RES_DIR / 'freq_bigram.json',\n",
    "                               RES_DIR / 'freq_emit.json')\n",
    "hmm.save(hmm_params_path)\n",
    "print('HMM 参数已保存:', hmm_params_path)\n",
    "# 预览部分 init / trans / emit 尺寸\n",
    "print('init 大小:', len(hmm.init_log_probs))\n",
    "print('trans 起始状态数:', len(hmm.trans_log_probs))\n",
    "print('emit 字数:', len(hmm.emit_log_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ae2047",
   "metadata": {},
   "source": [
    "## 4. 简单解码演示\n",
    "选取一串常见拼音（需在 base_pinyin_to_chars 中）。实际输出质量依赖语料规模与映射准确度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ff8ace3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> zhong guo ren min => 中国人民\n",
      "> ren min ri bao => 人民日报\n",
      "> jing ji fa zhan => 经济发展\n",
      "> xin wen bao gao => 新闻报告\n"
     ]
    }
   ],
   "source": [
    "def decode_sequence(py_str: str):\n",
    "    seq = py_str.split()\n",
    "    result = viterbi_decode(seq, base_map, hmm)\n",
    "    return seq, result\n",
    "for test in ['zhong guo ren min', 'ren min ri bao', 'jing ji fa zhan', 'xin wen bao gao']:\n",
    "    seq, out = decode_sequence(test)\n",
    "    print('>',' '.join(seq),'=>', out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4e2834",
   "metadata": {},
   "source": [
    "## 5. （可选）一致性快速校验\n",
    "确保 HMM 反序列化后结果一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18c6737d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reload check passed: 中国人\n"
     ]
    }
   ],
   "source": [
    "reloaded = HMMParams.load(hmm_params_path)\n",
    "test_seq = ['zhong','guo','ren']\n",
    "out1 = viterbi_decode(test_seq, base_map, hmm)\n",
    "out2 = viterbi_decode(test_seq, base_map, reloaded)\n",
    "assert out1 == out2, 'Reloaded HMM mismatch'\n",
    "print('Reload check passed:', out1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe121c6",
   "metadata": {},
   "source": [
    "## 6. 后续改进建议\n",
    "- 发射概率：利用 `word_to_pinyin_seq` 统计 (char,pinyin) 真实频率，而非复制式分配。\n",
    "- 引入 Top-K / Beam：限制每层扩展宽度提升速度。\n",
    "- 多音字截断：按 unigram 频次 / PMI 排序截取前 N。\n",
    "- 词级语言模型：对 解码结果 -> 分词 -> 语言模型重排序。\n",
    "- 评估：构建 (拼音输入, 参考汉字) 对，计算字符/词准确率。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
